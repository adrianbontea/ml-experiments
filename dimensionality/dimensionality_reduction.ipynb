{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mnist import *\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "training_set_path = \"D:\\\\Projects\\\\ml-experiments\\\\datasets\\\\mnist\\\\train-images-idx3-ubyte.gz\"\n",
    "train_labels_path = \"D:\\\\Projects\\\\ml-experiments\\\\datasets\\\\mnist\\\\train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "f_train = gzip.open(training_set_path)\n",
    "f_train_labels = gzip.open(train_labels_path)\n",
    "\n",
    "training_set = parse_idx(f_train)\n",
    "training_labels = parse_idx(f_train_labels)\n",
    "\n",
    "training_set_tr = training_set.reshape((60000, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How PCA works: For the number of dimensions that you want to reduce a dataset to, it identifies the axis for which the projection of the dataset onto generates the maximum amount of variance (or the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis based on Pythagoras' theorem) It starts with a first axis then finds a second axis orthogonal to the first that maximizes the amount of remaining variance and then a third axis orthogonal to the first two and so on - as many axes as the number of dimensions required to reduce the dataset to. The vectors that define the axis are called Principal Components. Once you have identified all the principal components, you can reduce the dimensionality of the dataset down to d dimensions by projecting it onto the hyperplane defined by the first d principal components.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set_tr[:1000, :]  # First 1000 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the dot product of three matrices U · Σ · VT, where VT contains all the principal components that we are looking for.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "\n",
    "# The principal components vectors are then the columns of the transpose of V matrix\n",
    "C1 = V.T[:, 0]   # Shape (784,)\n",
    "C2 = V.T[:, 1]   # Shape (784,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To project the training set onto the hyperplane, you can simply compute the dot product of the training set matrix X by the matrix Wd, defined as the matrix containing the first d principal components(i.e., the matrix composed of the first d columns of VT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " W2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(W2)\n",
    "\n",
    "# Same using Scikit-Learn\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)  # X2D should be identical to the one computed above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for data visualization—in that case you will generally want to reduce the dimensionality down to 2 or 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)  # The 1000 instances should now have 129 features instead of the original 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One problem with the preceding implementation of PCA is that it requires the whole training set to fit in memory in order for the SVD algorithm to run. Fortunately, Incremental PCA (IPCA) algorithms have been developed: you can split the training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is useful for large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set_tr\n",
    "\n",
    "n_batches = 100  # 100 batches of 600 instances\n",
    "inc_pca = IncrementalPCA(n_components=129)\n",
    "for X_batch in np.array_split(X, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_mnist_reduced = inc_pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measure the difference in the time required to train a K-Neighbors Classifier (known to be slow) on the original and reduced MNIST dataset...The difference should be huge!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a K-Neighbors Classifier on the original MNIST dataset took 22.77714490890503 seconds.\n",
      "Training a K-Neighbors Classifier on the reduced MNIST dataset took 0.6651895046234131 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X, training_labels)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Training a K-Neighbors Classifier on the original MNIST dataset took {elapsed} seconds.\")\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_mnist_reduced, training_labels)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Training a K-Neighbors Classifier on the reduced MNIST dataset took {elapsed} seconds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
