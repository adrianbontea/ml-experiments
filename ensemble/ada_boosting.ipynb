{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "from mnist import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "training_set_path = \"D:\\\\Projects\\\\ml-experiments\\\\datasets\\\\mnist\\\\train-images-idx3-ubyte.gz\"\n",
    "train_labels_path = \"D:\\\\Projects\\\\ml-experiments\\\\datasets\\\\mnist\\\\train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "f_train = gzip.open(training_set_path)\n",
    "f_train_labels = gzip.open(train_labels_path)\n",
    "\n",
    "training_set = parse_idx(f_train)\n",
    "training_labels = parse_idx(f_train_labels)\n",
    "\n",
    "training_set_tr = training_set.reshape((60000, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some utility function to reuse throughout experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_digit(training_set, labels, digit):\n",
    "    indexes = np.where(labels == digit)[0]\n",
    "    return training_set[indexes[np.random.randint(0, len(indexes) - 1)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each instance weight $w_{(i)}$ is initially set to $\\frac{1}{m}$ so given equal weight (m is the number of instances). A first predictor is trained and its WEIGHTED ERROR RATE $r_1$ is computed on the training set: $r_j = \\frac {\\sum_{i=1 y_j^{(i)}!=y^{(i)}}^m w^{(i)}}{\\sum_{i=1}^m w^{(i)}}$ ($y_j^{(i)}$ is the jth predictor prediction for instance i)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next the predictor's weight is then computed: $\\alpha_j = \\eta * \\log \\frac{1-r_j}{r_j}$  ($\\eta$ is the learning rate) In machine learning, the logarithm or exponential without a base specified often refers to the natural log/exp so base is e**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next the instance weights are updated and the misclassified instances are boosted: for i = 1->m, if $y_j^{(i)} == y^{(i)}$ $w^{(i)} -> w{(i)}$, otherwise $w^{(i)} -> w^{(i)} * \\exp (\\alpha_j)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, a new predictor is trained using the updated weights, and the whole process is repeated (the new predictor’s weight is computed, the instance weights are updated, then another predictor is trained, and so on). The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found. To make predictions, AdaBoost simply computes the predictions of all the predictors and weighs them using the predictor weights αj. The predicted class is the one that receives the majority of weighted votes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training an AdaBoostClassifier with 10 stumps took 12.799814701080322 seconds\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=10,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "\n",
    "start_time = time.time()\n",
    "ada_clf.fit(training_set_tr, training_labels)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Training an AdaBoostClassifier with 10 stumps took {elapsed} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME which stands for Stagewise Additive Modeling using a Multiclass Exponential loss function. In this case since the DecisionTree estimator/predictor can estimate class probabilities (exposes a predict_proba method) Scikit-Learn can use a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class probabilities rather than predictions and generally performs better.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Decision Tree with max_depth=1, in other words, a tree composed of a single decision node plus two leaf nodes is called a Decision Stump**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier prediction is: [8]\n"
     ]
    }
   ],
   "source": [
    "an_eight = get_random_digit(training_set_tr, training_labels, 8)\n",
    "\n",
    "print(f\"AdaBoostClassifier prediction is: {ada_clf.predict([an_eight])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
