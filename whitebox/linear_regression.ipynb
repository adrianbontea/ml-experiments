{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate some linear-looking data to test the normal equation. Note: X is a ndarray of shape (100,1) simulating a training set of 100 instances with one feature each. y is a ndarray of shape (100,1) simulating the labels for the 100 training instances. y is a linear-ish model introducing some noise (+ np.random.uniform(1, 5, (100, 1)) to the linear equation (y = ax + b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = 2 * np.random.randn(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "plt.plot(X, y, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal equation: $\\Theta=(X^T*X)^{-1}*X^T*y$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The equation gives the value of Theta that minimizes the MSE cost function. Note the inverse of a matrix A noted $A^{-1}$ is a matrix in such a way that $A*A^{-1}=I$ (identity matrix = 1 on the main diagonal and all other elements 0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add bias X0 = 1 to each instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters vector as determined by the normal equation:[[3.88323508]\n",
      " [2.98912288]]\n"
     ]
    }
   ],
   "source": [
    "X_b = np.append(np.ones((100, 1)), X, axis=1)\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(f\"Parameters vector as determined by the normal equation:{theta_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the parameters computed via normal equation to make predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions based on parameters computed via normal equation:[[ 6.87235796]\n",
      " [12.85060371]]\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([[1], [3]])\n",
    "X_new_b = np.append(np.ones((2, 1)), X_new, axis=1)\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "\n",
    "print(f\"Predictions based on parameters computed via normal equation:{y_predict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the same using Scikit-Learn and compare - the predictions should be identical!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions done by Scikit-Learn:[[ 6.87235796]\n",
      " [12.85060371]]\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "print(f\"Predictions done by Scikit-Learn:{lin_reg.predict(X_new)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Normal Equation gets very slow when the number of features grows large! On the positive side, this equation is linear with regards to the number of instances in the training set (it is O(m)), so it handles large training sets efficiently, provided they can fit in memory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Also, once you have trained your Linear Regression model, predictions are very fast: the computational complexity is linear with regards to both the number of instances you want to make predictions on and the number of features. In other words, making predictions on twice as many instances (or twice as many features) will just take roughly twice as much time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the training set (determine theta best) using Batch Gradient Descent. The result should be the same as normal equation. Batch gradient descent involves computing the partial derivatives of the MSE cost function with respect to each parameter and then the value of the derivatives in point theta (coordinates) at each training step! That is because the derivative of a function determines the slope of the tangent to the function curve in a certain point. Hence these partial derivatives are about determining the slope of the cost function with regards to each axis represented by each model parameter, trying to reach a global minimum for the cost function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For a certain parameter $\\theta_j$, the partial derivative (gradient) will be: $\\frac{2}{m}*\\sum_{i=1}^m (\\theta^T*x^{(i)}-y^{(i)})*x_j^{(i)}$ (feature j from instance i)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A vector of all these gradients for the whole training set: $\\frac{2}{m}*X^T*(X*\\theta - y)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is why the algorithm is called Batch Gradient Descent: it uses the whole batch of training data at every step. As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of featuresis much faster using Gradient Descent than using the Normal Equation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters vector as determined by Batch Gradient Descent:[[3.88323508]\n",
      " [2.98912288]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2 / m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(f\"Parameters vector as determined by Batch Gradient Descent:{theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These are precisely the values determined by the normal equation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch Gradient Descent is very slow when the training set is large as it uses the whole set on each iteration. Stochastic (Random) Gradient Descent is a similar algorithm but uses a single instance, randomly chosen from the training set, on each iteration, to compute the gradients vector and theta next. It is much faster but less accurate. To avoid bouncing around after finding the best parameters vector that minimizes the MSE function, the eta (learning rate) is gradually reduced in a process called learning schedule. This process is called simulated annealing, because it resembles the process of annealing in metallurgy where molten metal is slowly cooled down. By convention we iterate by rounds of m iterations; each round is called an epoch. While the Batch Gradient Descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times and reaches a fairly good solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters vector as determined by Stochastic Gradient Descent:[[3.88788083]\n",
      " [3.01708896]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "def learning_schedule(t):  # Local function\n",
    "    t0, t1 = 5, 50\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = np.array([X_b[random_index, ]])\n",
    "        yi = np.array([y[random_index, ]])\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)  # This will gradually reduce eta on each epoch as i increases and globally as epoch increases\n",
    "        theta = theta - eta * gradients\n",
    "\n",
    "print(f\"Parameters vector as determined by Stochastic Gradient Descent:{theta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Min-Batch Gradient Descent is a common ground between Batch and Stochastic Gradient Decent algorithms. At each training step will compute the gradients based on a mini batch from the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters vector as determined by Mini-Batch Gradient Descent:[[3.8592836 ]\n",
      " [2.98391042]]\n"
     ]
    }
   ],
   "source": [
    "mini_batch_size = 50\n",
    "n_iterations = 500\n",
    "eta = 0.1  # learning rate\n",
    "theta = np.random.randn(2, 1)  # random initialization\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    from_index = np.random.randint(0, 74)\n",
    "    to_index = from_index + mini_batch_size\n",
    "    mini_batch_x = X_b[from_index:to_index, :]\n",
    "    mini_batch_y = y[from_index:to_index, :]\n",
    "    gradients = 2 / mini_batch_size * mini_batch_x.T.dot(mini_batch_x.dot(theta) - mini_batch_y)\n",
    "    theta = theta - eta * gradients\n",
    "\n",
    "print(f\"Parameters vector as determined by Mini-Batch Gradient Descent:{theta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
